"""
Core Storage Operations

ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ»å–å¾—ã«é–¢ã™ã‚‹å…±é€šæ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import pandas as pd
import logging
from pathlib import Path
from typing import List, Dict, Optional


def save_to_parquet(
    data: List[Dict], table_name: str, partition_cols: Optional[List[str]] = None
) -> str:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚’Parquetå½¢å¼ã§ä¿å­˜

    Args:
        data: ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        partition_cols: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³åˆ—ã®ãƒªã‚¹ãƒˆ

    Returns:
        ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºä¿
    base_dir = Path("/opt/airflow/data/processed")
    base_dir.mkdir(parents=True, exist_ok=True)

    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ å¤‰æ›
    df = pd.DataFrame(data)

    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹æ±ºå®š
    from datetime import datetime

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{table_name}_{timestamp}.parquet"
    file_path = base_dir / filename

    # Parquetä¿å­˜
    if partition_cols and all(col in df.columns for col in partition_cols):
        # ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ä¿å­˜
        partition_dir = base_dir / f"{table_name}_partitioned_{timestamp}"
        df.to_parquet(partition_dir, partition_cols=partition_cols, index=False)
        logging.info(
            f"Saved partitioned parquet: {partition_dir} ({len(df)} records)"
        )
        return str(partition_dir)
    else:
        # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        df.to_parquet(file_path, index=False)
        logging.info(f"Saved parquet file: {filename} ({len(df)} records)")
        return str(file_path)


def get_parquet_path(table_name: str) -> Optional[str]:
    """
    ãƒ†ãƒ¼ãƒ–ãƒ«åã‹ã‚‰æœ€æ–°ã®Parquetãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—

    Args:
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å

    Returns:
        æœ€æ–°ã®Parquetãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    base_dir = Path("/opt/airflow/data/processed")

    # ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒã§ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢
    parquet_files = list(base_dir.glob(f"{table_name}_*.parquet"))
    parquet_dirs = list(base_dir.glob(f"{table_name}_partitioned_*"))

    all_paths = parquet_files + parquet_dirs

    if not all_paths:
        logging.warning(f"No parquet files found for table: {table_name}")
        return None

    # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿”ã™ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—é †ï¼‰
    latest_path = max(all_paths, key=lambda p: p.stat().st_mtime)
    return str(latest_path)


def load_from_parquet(file_path: str) -> pd.DataFrame:
    """
    Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿

    Args:
        file_path: Parquetãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
    """
    try:
        df = pd.read_parquet(file_path)
        logging.info(f"Loaded parquet file: {file_path} ({len(df)} records)")
        return df
    except Exception as e:
        logging.error(f"Failed to load parquet file {file_path}: {str(e)}")
        raise


def cleanup_old_files(table_name: str, keep_count: int = 3) -> None:
    """
    å¤ã„Parquetãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

    Args:
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
        keep_count: ä¿æŒã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ•°
    """
    base_dir = Path("/opt/airflow/data/processed")

    # è©²å½“ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
    parquet_files = list(base_dir.glob(f"{table_name}_*.parquet"))
    parquet_dirs = list(base_dir.glob(f"{table_name}_partitioned_*"))

    all_paths = parquet_files + parquet_dirs

    if len(all_paths) <= keep_count:
        return

    # å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å‰Šé™¤
    sorted_paths = sorted(all_paths, key=lambda p: p.stat().st_mtime, reverse=True)
    files_to_remove = sorted_paths[keep_count:]

    removed_count = 0
    for path_to_remove in files_to_remove:
        try:
            if path_to_remove.is_dir():
                import shutil

                shutil.rmtree(path_to_remove)
            else:
                path_to_remove.unlink()
            removed_count += 1
        except Exception as e:
            logging.warning(f"Failed to remove {path_to_remove}: {str(e)}")

    if removed_count > 0:
        logging.info(f"ğŸ—‘ï¸ Cleaned up {removed_count} old {table_name} files")


def get_storage_stats() -> Dict:
    """
    ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸çµ±è¨ˆæƒ…å ±ã‚’å–å¾—

    Returns:
        ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸çµ±è¨ˆè¾æ›¸
    """
    base_dir = Path("/opt/airflow/data")

    stats = {
        "raw_files": 0,
        "processed_files": 0,
        "total_size_mb": 0,
        "directories": [],
    }

    # å„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®çµ±è¨ˆ
    for subdir in ["raw", "processed", "curated"]:
        dir_path = base_dir / subdir
        if dir_path.exists():
            files = list(dir_path.rglob("*"))
            file_count = len([f for f in files if f.is_file()])

            # ã‚µã‚¤ã‚ºè¨ˆç®—
            total_size = sum(f.stat().st_size for f in files if f.is_file())
            size_mb = total_size / (1024 * 1024)

            stats["directories"].append(
                {"name": subdir, "file_count": file_count, "size_mb": round(size_mb, 2)}
            )

            if subdir == "raw":
                stats["raw_files"] = file_count
            elif subdir == "processed":
                stats["processed_files"] = file_count

            stats["total_size_mb"] += size_mb

    stats["total_size_mb"] = round(stats["total_size_mb"], 2)

    return stats


def ensure_data_directories() -> None:
    """
    å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    """
    base_dir = Path("/opt/airflow/data")

    directories = ["raw/crime", "raw/school", "processed", "curated"]

    for directory in directories:
        dir_path = base_dir / directory
        dir_path.mkdir(parents=True, exist_ok=True)
        logging.info(f"ğŸ“ Ensured directory: {dir_path}")
