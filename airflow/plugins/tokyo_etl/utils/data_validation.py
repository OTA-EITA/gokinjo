"""
Data Validation Utilities

çŠ¯ç½ªãƒ‡ãƒ¼ã‚¿ãƒ»å­¦æ ¡ãƒ‡ãƒ¼ã‚¿ã®å“è³ªãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ã€‚
ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¾Œã®ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§æ¤œè¨¼ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import pandas as pd
import logging
from typing import Dict, List


def validate_crime_data(file_paths: List[str]) -> Dict:
    """
    çŠ¯ç½ªãƒ‡ãƒ¼ã‚¿ã®å“è³ªãƒã‚§ãƒƒã‚¯

    Args:
        file_paths: ãƒã‚§ãƒƒã‚¯å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ

    Returns:
        å“è³ªãƒã‚§ãƒƒã‚¯çµæœè¾æ›¸
    """
    logging.info(f"Starting crime data validation for {len(file_paths)} files")

    results = {
        "total_files": len(file_paths),
        "valid_files": 0,
        "total_records": 0,
        "valid_records": 0,
        "quality_issues": [],
        "score": 0.0,
    }

    for file_path in file_paths:
        try:
            file_result = _validate_single_crime_file(file_path)
            results["valid_files"] += file_result["is_valid"]
            results["total_records"] += file_result["total_records"]
            results["valid_records"] += file_result["valid_records"]
            results["quality_issues"].extend(file_result["issues"])

        except Exception as e:
            error_msg = f"Failed to validate {file_path}: {str(e)}"
            results["quality_issues"].append(error_msg)
            logging.error(f"{error_msg}")

    # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
    if results["total_records"] > 0:
        record_quality = results["valid_records"] / results["total_records"]
        file_quality = (
            results["valid_files"] / results["total_files"]
            if results["total_files"] > 0
            else 0
        )
        results["score"] = (record_quality * 0.7 + file_quality * 0.3) * 100

    logging.info(
        f"ğŸ“Š Crime data validation complete: {results['score']:.1f}% quality score"
    )
    return results


def validate_school_data(file_paths: List[str]) -> Dict:
    """
    å­¦æ ¡ãƒ‡ãƒ¼ã‚¿ã®å“è³ªãƒã‚§ãƒƒã‚¯

    Args:
        file_paths: ãƒã‚§ãƒƒã‚¯å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ

    Returns:
        å“è³ªãƒã‚§ãƒƒã‚¯çµæœè¾æ›¸
    """
    logging.info(f"Starting school data validation for {len(file_paths)} files")

    results = {
        "total_files": len(file_paths),
        "valid_files": 0,
        "total_records": 0,
        "valid_records": 0,
        "quality_issues": [],
        "score": 0.0,
    }

    for file_path in file_paths:
        try:
            file_result = _validate_single_school_file(file_path)
            results["valid_files"] += file_result["is_valid"]
            results["total_records"] += file_result["total_records"]
            results["valid_records"] += file_result["valid_records"]
            results["quality_issues"].extend(file_result["issues"])

        except Exception as e:
            error_msg = f"Failed to validate {file_path}: {str(e)}"
            results["quality_issues"].append(error_msg)
            logging.error(f"{error_msg}")

    # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
    if results["total_records"] > 0:
        record_quality = results["valid_records"] / results["total_records"]
        file_quality = (
            results["valid_files"] / results["total_files"]
            if results["total_files"] > 0
            else 0
        )
        results["score"] = (record_quality * 0.7 + file_quality * 0.3) * 100

    logging.info(
        f"ğŸ“Š School data validation complete: {results['score']:.1f}% quality score"
    )
    return results


def _validate_single_crime_file(file_path: str) -> Dict:
    """
    å˜ä¸€çŠ¯ç½ªãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®å“è³ªãƒã‚§ãƒƒã‚¯
    """
    issues = []

    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    try:
        df = pd.read_csv(file_path, encoding="shift_jis")
    except UnicodeDecodeError:
        try:
            df = pd.read_csv(file_path, encoding="utf-8")
        except Exception as e:
            return {
                "is_valid": False,
                "total_records": 0,
                "valid_records": 0,
                "issues": [f"Failed to read file {file_path}: {str(e)}"],
            }

    total_records = len(df)
    valid_records = 0

    # å¿…é ˆåˆ—ãƒã‚§ãƒƒã‚¯
    required_columns = ["äº‹ä»¶ç¨®åˆ¥", "ç™ºç”Ÿæ—¥æ™‚", "ç·¯åº¦", "çµŒåº¦"]
    missing_columns = [col for col in required_columns if col not in df.columns]

    if missing_columns:
        issues.append(f"Missing required columns: {missing_columns}")
        return {
            "is_valid": False,
            "total_records": total_records,
            "valid_records": 0,
            "issues": issues,
        }

    # ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã®æ¤œè¨¼
    for index, row in df.iterrows():
        record_valid = True

        # åº§æ¨™ç¯„å›²ãƒã‚§ãƒƒã‚¯ï¼ˆæ±äº¬23åŒºå‘¨è¾ºï¼‰
        if not (35.5 <= row["ç·¯åº¦"] <= 35.9 and 139.3 <= row["çµŒåº¦"] <= 139.9):
            record_valid = False

        # äº‹ä»¶ç¨®åˆ¥ãƒã‚§ãƒƒã‚¯
        if pd.isna(row["äº‹ä»¶ç¨®åˆ¥"]) or str(row["äº‹ä»¶ç¨®åˆ¥"]).strip() == "":
            record_valid = False

        # æ—¥æ™‚ãƒã‚§ãƒƒã‚¯
        try:
            pd.to_datetime(row["ç™ºç”Ÿæ—¥æ™‚"])
        except (ValueError, TypeError, pd.errors.ParserError):
            record_valid = False

        if record_valid:
            valid_records += 1

    # å“è³ªå•é¡Œã®è¨˜éŒ²
    if valid_records < total_records * 0.8:
        issues.append(
            f"Low record quality: {valid_records}/{total_records} valid records"
        )

    return {
        "is_valid": len(issues) == 0 and valid_records > 0,
        "total_records": total_records,
        "valid_records": valid_records,
        "issues": issues,
    }


def _validate_single_school_file(file_path: str) -> Dict:
    """
    å˜ä¸€å­¦æ ¡ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®å“è³ªãƒã‚§ãƒƒã‚¯
    """
    issues = []

    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    try:
        df = pd.read_csv(file_path, encoding="utf-8")
    except UnicodeDecodeError:
        try:
            df = pd.read_csv(file_path, encoding="shift_jis")
        except Exception as e:
            return {
                "is_valid": False,
                "total_records": 0,
                "valid_records": 0,
                "issues": [f"Failed to read file {file_path}: {str(e)}"],
            }

    total_records = len(df)
    valid_records = 0

    # å¿…é ˆåˆ—ãƒã‚§ãƒƒã‚¯
    required_columns = ["å­¦æ ¡å", "å­¦æ ¡ç¨®åˆ¥", "ç·¯åº¦", "çµŒåº¦"]
    missing_columns = [col for col in required_columns if col not in df.columns]

    if missing_columns:
        issues.append(f"Missing required columns: {missing_columns}")
        return {
            "is_valid": False,
            "total_records": total_records,
            "valid_records": 0,
            "issues": issues,
        }

    # ãƒ¬ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã®æ¤œè¨¼
    for index, row in df.iterrows():
        record_valid = True

        # åº§æ¨™ç¯„å›²ãƒã‚§ãƒƒã‚¯ï¼ˆæ±äº¬23åŒºå‘¨è¾ºï¼‰
        if not (35.5 <= row["ç·¯åº¦"] <= 35.9 and 139.3 <= row["çµŒåº¦"] <= 139.9):
            record_valid = False

        # å­¦æ ¡åãƒã‚§ãƒƒã‚¯
        if pd.isna(row["å­¦æ ¡å"]) or str(row["å­¦æ ¡å"]).strip() == "":
            record_valid = False

        # å­¦æ ¡ç¨®åˆ¥ãƒã‚§ãƒƒã‚¯
        valid_school_types = ["å°å­¦æ ¡", "ä¸­å­¦æ ¡", "é«˜ç­‰å­¦æ ¡", "é«˜æ ¡"]
        if str(row["å­¦æ ¡ç¨®åˆ¥"]) not in valid_school_types:
            record_valid = False

        if record_valid:
            valid_records += 1

    # å“è³ªå•é¡Œã®è¨˜éŒ²
    if valid_records < total_records * 0.8:
        issues.append(
            f"Low record quality: {valid_records}/{total_records} valid records"
        )

    return {
        "is_valid": len(issues) == 0 and valid_records > 0,
        "total_records": total_records,
        "valid_records": valid_records,
        "issues": issues,
    }


def check_coordinate_accuracy(df: pd.DataFrame, lat_col: str, lng_col: str) -> Dict:
    """
    åº§æ¨™ç²¾åº¦ãƒã‚§ãƒƒã‚¯

    Args:
        df: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        lat_col: ç·¯åº¦åˆ—å
        lng_col: çµŒåº¦åˆ—å

    Returns:
        åº§æ¨™ç²¾åº¦ãƒã‚§ãƒƒã‚¯çµæœ
    """
    results = {
        "total_coordinates": len(df),
        "valid_coordinates": 0,
        "precision_levels": {},
        "outliers": [],
    }

    for index, row in df.iterrows():
        lat, lng = row[lat_col], row[lng_col]

        # æœ‰åŠ¹æ€§ãƒã‚§ãƒƒã‚¯
        if pd.notna(lat) and pd.notna(lng):
            if 35.0 <= lat <= 36.0 and 139.0 <= lng <= 140.0:
                results["valid_coordinates"] += 1

                # ç²¾åº¦ãƒ¬ãƒ™ãƒ«åˆ¤å®š
                lat_str, lng_str = str(lat), str(lng)
                decimal_places = min(
                    len(lat_str.split(".")[-1]) if "." in lat_str else 0,
                    len(lng_str.split(".")[-1]) if "." in lng_str else 0,
                )

                precision_key = f"{decimal_places}_digits"
                results["precision_levels"][precision_key] = (
                    results["precision_levels"].get(precision_key, 0) + 1
                )
            else:
                results["outliers"].append(
                    {
                        "index": index,
                        "lat": lat,
                        "lng": lng,
                        "reason": "outside_tokyo_area",
                    }
                )

    return results


def validate_date_consistency(df: pd.DataFrame, date_col: str) -> Dict:
    """
    æ—¥ä»˜ãƒ‡ãƒ¼ã‚¿ã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯

    Args:
        df: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ 
        date_col: æ—¥ä»˜åˆ—å

    Returns:
        æ—¥ä»˜ãƒã‚§ãƒƒã‚¯çµæœ
    """
    results = {
        "total_dates": len(df),
        "valid_dates": 0,
        "date_range": {},
        "format_issues": [],
    }

    valid_dates = []

    for index, row in df.iterrows():
        date_value = row[date_col]

        try:
            parsed_date = pd.to_datetime(date_value)
            valid_dates.append(parsed_date)
            results["valid_dates"] += 1
        except (ValueError, TypeError, pd.errors.ParserError):
            results["format_issues"].append(
                {"index": index, "value": date_value, "issue": "unparseable_date"}
            )

    if valid_dates:
        results["date_range"] = {
            "earliest": min(valid_dates).isoformat(),
            "latest": max(valid_dates).isoformat(),
            "span_days": (max(valid_dates) - min(valid_dates)).days,
        }

    return results


def generate_data_quality_report(validation_results: Dict, data_type: str) -> str:
    """
    ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ

    Args:
        validation_results: å“è³ªãƒã‚§ãƒƒã‚¯çµæœ
        data_type: ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—

    Returns:
        å“è³ªãƒ¬ãƒãƒ¼ãƒˆï¼ˆãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ï¼‰
    """
    report = f"""
ğŸ“Š {data_type.upper()} DATA QUALITY REPORT

Overall Quality Score: {validation_results["score"]:.1f}%

File Statistics:
â€¢ Total Files: {validation_results["total_files"]}
â€¢ Valid Files: {validation_results["valid_files"]}
â€¢ File Success Rate: {(validation_results["valid_files"] / validation_results["total_files"] * 100):.1f}%

Record Statistics:
â€¢ Total Records: {validation_results["total_records"]:,}
â€¢ Valid Records: {validation_results["valid_records"]:,}
â€¢ Record Success Rate: {(validation_results["valid_records"] / validation_results["total_records"] * 100 if validation_results["total_records"] > 0 else 0):.1f}%

Quality Issues:
"""

    if validation_results["quality_issues"]:
        for i, issue in enumerate(validation_results["quality_issues"][:10], 1):
            report += f"â€¢ {issue}\n"

        if len(validation_results["quality_issues"]) > 10:
            report += f"... and {len(validation_results['quality_issues']) - 10} more issues\n"
    else:
        report += "â€¢ No significant quality issues detected âœ…\n"

    report += f"\nGenerated: {pd.Timestamp.now().isoformat()}"

    return report.strip()
