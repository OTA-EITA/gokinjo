"""
Data Download Utilities

å„ç¨®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ã€‚
çŠ¯ç½ªãƒ‡ãƒ¼ã‚¿ãƒ»å­¦æ ¡ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã‚’çµ±ä¸€çš„ã«å‡¦ç†ã—ã¾ã™ã€‚
"""

import os
import requests
import logging
from typing import List
from datetime import datetime
from pathlib import Path


def download_from_sources(
    source_urls: List[str], data_type: str = "general"
) -> List[str]:
    """
    è¤‡æ•°ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

    Args:
        source_urls: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¯¾è±¡URLã®ãƒªã‚¹ãƒˆ
        data_type: ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ï¼ˆãƒ­ã‚°ç”¨ï¼‰

    Returns:
        ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
    """
    downloaded_files = []
    download_dir = _get_download_directory(data_type)

    logging.info(
        f"ğŸ“¥ Starting download of {data_type} data from {len(source_urls)} sources"
    )

    for i, url in enumerate(source_urls):
        try:
            file_path = download_single_file(url, download_dir, data_type, i)
            if file_path:
                downloaded_files.append(file_path)
                logging.info(f"Downloaded: {file_path}")
        except Exception as e:
            logging.error(f"Failed to download {url}: {str(e)}")
            # ä¸€ã¤ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤±æ•—ã—ã¦ã‚‚ä»–ã¯ç¶šè¡Œ
            continue

    logging.info(
        f"ğŸ“Š Download summary: {len(downloaded_files)}/{len(source_urls)} files successful"
    )
    return downloaded_files


def download_single_file(
    url: str, download_dir: Path, data_type: str, index: int = 0
) -> str:
    """
    å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

    Args:
        url: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URL
        download_dir: ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        data_type: ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—
        index: ãƒ•ã‚¡ã‚¤ãƒ«ç•ªå·

    Returns:
        ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    """
    # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{data_type}_{timestamp}_{index}.csv"
    file_path = download_dir / filename

    # HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆè¨­å®š
    headers = {
        "User-Agent": "Tokyo-Crime-School-ETL/1.0 (Data Integration Purpose)",
        "Accept": "text/csv,application/csv,text/plain,*/*",
    }

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Ÿè¡Œ
    response = requests.get(url, headers=headers, stream=True, timeout=30)
    response.raise_for_status()

    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    with open(file_path, "wb") as f:
        for chunk in response.iter_content(chunk_size=8192):
            f.write(chunk)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
    file_size = os.path.getsize(file_path)
    logging.info(f"ğŸ“ File saved: {filename} ({file_size:,} bytes)")

    return str(file_path)


def download_with_retry(url: str, max_retries: int = 3) -> requests.Response:
    """
    ãƒªãƒˆãƒ©ã‚¤æ©Ÿèƒ½ä»˜ããƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

    Args:
        url: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URL
        max_retries: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°

    Returns:
        HTTPãƒ¬ã‚¹ãƒãƒ³ã‚¹
    """
    for attempt in range(max_retries + 1):
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            return response
        except requests.RequestException as e:
            if attempt < max_retries:
                wait_time = 2**attempt  # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•
                logging.warning(
                    f"Download attempt {attempt + 1} failed, retrying in {wait_time}s: {str(e)}"
                )
                import time

                time.sleep(wait_time)
            else:
                logging.error(f"All download attempts failed for {url}")
                raise


def verify_download_integrity(file_path: str, expected_min_size: int = 100) -> bool:
    """
    ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ç¢ºèª

    Args:
        file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        expected_min_size: æœŸå¾…ã™ã‚‹æœ€å°ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º

    Returns:
        æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯çµæœ
    """
    try:
        if not os.path.exists(file_path):
            logging.error(f"File not found: {file_path}")
            return False

        file_size = os.path.getsize(file_path)
        if file_size < expected_min_size:
            logging.error(f"File too small: {file_path} ({file_size} bytes)")
            return False

        # CSVã®åŸºæœ¬çš„ãªæ§‹é€ ãƒã‚§ãƒƒã‚¯
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            first_line = f.readline().strip()
            if not first_line or "," not in first_line:
                logging.error(f"Invalid CSV structure: {file_path}")
                return False

        logging.info(f"File integrity verified: {file_path}")
        return True

    except Exception as e:
        logging.error(f"File integrity check failed: {file_path} - {str(e)}")
        return False


def cleanup_old_downloads(data_type: str, keep_days: int = 7) -> None:
    """
    å¤ã„ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—

    Args:
        data_type: ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—
        keep_days: ä¿æŒæ—¥æ•°
    """
    download_dir = _get_download_directory(data_type)
    cutoff_time = datetime.now().timestamp() - (keep_days * 24 * 3600)

    cleaned_count = 0
    for file_path in download_dir.glob(f"{data_type}_*.csv"):
        try:
            if file_path.stat().st_mtime < cutoff_time:
                file_path.unlink()
                cleaned_count += 1
        except Exception as e:
            logging.warning(f"Failed to cleanup {file_path}: {str(e)}")

    if cleaned_count > 0:
        logging.info(f"Cleaned up {cleaned_count} old {data_type} files")


def _get_download_directory(data_type: str) -> Path:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—åˆ¥ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—

    Args:
        data_type: ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—

    Returns:
        ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®Path
    """
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹
    base_dir = Path("/opt/airflow/data/raw") / data_type
    base_dir.mkdir(parents=True, exist_ok=True)
    return base_dir


def get_mock_crime_data() -> List[str]:
    """
    é–‹ç™ºç”¨ï¼šãƒ¢ãƒƒã‚¯çŠ¯ç½ªãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ

    Returns:
        ä½œæˆã—ãŸãƒ¢ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ãƒªã‚¹ãƒˆ
    """
    mock_data = """äº‹ä»¶ç¨®åˆ¥,ç™ºç”Ÿæ—¥æ™‚,ç™ºç”Ÿå ´æ‰€,ç·¯åº¦,çµŒåº¦
ä¾µå…¥çªƒç›—,2024-01-15 10:30:00,å°æ±åŒºæµ…è‰1-1-1,35.7148,139.7967
è·¯ä¸Šå¼·ç›—,2024-01-16 22:15:00,æ–‡äº¬åŒºæœ¬éƒ·3-3-3,35.7089,139.7624
è‡ªè»¢è»Šç›—,2024-01-17 14:20:00,å°æ±åŒºä¸Šé‡2-2-2,35.7090,139.7740
"""

    download_dir = _get_download_directory("crime")
    mock_file = (
        download_dir / f"mock_crime_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    )

    with open(mock_file, "w", encoding="shift_jis") as f:
        f.write(mock_data)

    logging.info(f"ğŸ“ Created mock crime data: {mock_file}")
    return [str(mock_file)]


def get_mock_school_data() -> List[str]:
    """
    é–‹ç™ºç”¨ï¼šãƒ¢ãƒƒã‚¯å­¦æ ¡ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ

    Returns:
        ä½œæˆã—ãŸãƒ¢ãƒƒã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ãƒªã‚¹ãƒˆ
    """
    mock_data = """å­¦æ ¡å,å­¦æ ¡ç¨®åˆ¥,ä½æ‰€,ç·¯åº¦,çµŒåº¦,è¨­ç½®è€…
å°æ±åŒºç«‹æµ…è‰å°å­¦æ ¡,å°å­¦æ ¡,å°æ±åŒºæµ…è‰1-1-1,35.7148,139.7967,åŒºç«‹
æ–‡äº¬åŒºç«‹æœ¬éƒ·ä¸­å­¦æ ¡,ä¸­å­¦æ ¡,æ–‡äº¬åŒºæœ¬éƒ·3-3-3,35.7089,139.7624,åŒºç«‹
æ±äº¬éƒ½ç«‹ä¸Šé‡é«˜ç­‰å­¦æ ¡,é«˜ç­‰å­¦æ ¡,å°æ±åŒºä¸Šé‡2-2-2,35.7090,139.7740,éƒ½ç«‹
"""

    download_dir = _get_download_directory("school")
    mock_file = (
        download_dir
        / f"mock_school_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    )

    with open(mock_file, "w", encoding="utf-8") as f:
        f.write(mock_data)

    logging.info(f"ğŸ“ Created mock school data: {mock_file}")
    return [str(mock_file)]
